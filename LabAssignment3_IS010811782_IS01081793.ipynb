{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c6ea01",
   "metadata": {},
   "source": [
    "Name : Ahmad Nabil Bin Yusoff - IS01081782\n",
    "Name : Ikmal Kamil Bin Mohd kamil - IS01081793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f4e79",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "707d1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amakn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amakn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amakn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b030e4",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a46616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "df = pd.read_csv('news_dataset.csv')\n",
    "\n",
    " # Remove rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "documents = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db92b5",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "100c68a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the Data\n",
    "stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize a WordNet lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Filter out non-alphanumeric tokens\n",
    "    \n",
    "    # Remove numbers and single-character tokens\n",
    "    tokens = [token for token in tokens if token.isalpha() and len(token) > 1]\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each token\n",
    "    return tokens  # Return the preprocessed tokens\n",
    "\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]  # Preprocess each document in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ec7ee",
   "metadata": {},
   "source": [
    "## Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed4614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents) \n",
    "\n",
    "# Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a7706",
   "metadata": {},
   "source": [
    "## Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60be2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)  # Train an LDA model on the corpus with 2 topics using Gensim's LdaModel class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce8ad4",
   "metadata": {},
   "source": [
    "## Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b36a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret Results\n",
    "\n",
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append to the list\n",
    "    article_labels.append(dominant_topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "080e7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      I was wondering if anyone out there could enli...      0\n",
      "1      I recently posted an article asking what kind ...      0\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      0\n",
      "3      an excellent automatic can be found in the sub...      0\n",
      "4      : Ford and his automobile.  I need information...      0\n",
      "...                                                  ...    ...\n",
      "11091  Secrecy in Clipper Chip\\n\\nThe serial number o...      2\n",
      "11092  Hi !\\n\\nI am interested in the source of FEAL ...      2\n",
      "11093  The actual algorithm is classified, however, t...      0\n",
      "11094  \\n\\tThis appears to be generic calling upon th...      0\n",
      "11095  \\nProbably keep quiet and take it, lest they g...      0\n",
      "\n",
      "[11096 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e755aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['would', 'one', 'get', 'like', 'know', 'think', 'good', 'time', 'year', 'could']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['people', 'would', 'one', 'government', 'say', 'think', 'god', 'u', 'right', 'law']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['key', 'file', 'use', 'system', 'chip', 'encryption', 'program', 'window', 'information', 'data']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['max', 'db', 'team', 'university', 'game', 'year', 'new', 'space', 'league', 'season']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Top terms for Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b54ccc",
   "metadata": {},
   "source": [
    "Topic 0 seems to be centered around personal development and life experiences, where the prominence of terms like \"would,\" \"think,\" and \"time\" are particularly high, indicating their significance in discussions about self-improvement and life reflections.\n",
    "\n",
    "Topic 1 appears to be related to social and political issues, with a strong emphasis on terms like \"government,\" \"law,\" and \"rights.\" These terms are relatively high in weight, suggesting a significant association with discussions on governance and societal norms.\n",
    "\n",
    "Topic 2 is clearly focused on technology and security, where terms such as \"encryption,\" \"system,\" and \"data\" carry considerable weight, reflecting their crucial role in conversations about cybersecurity and technological advancements.\n",
    "\n",
    "Topic 3 seems to be connected to sports and academia, particularly with a high incidence of terms like \"team,\" \"university,\" and \"game.\" These terms indicate a strong link to discussions about university sports and possibly the statistical analysis within sports contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "818e1d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"would\" (weight: 0.013)\n",
      "- \"one\" (weight: 0.011)\n",
      "- \"get\" (weight: 0.011)\n",
      "- \"like\" (weight: 0.009)\n",
      "- \"know\" (weight: 0.008)\n",
      "- \"think\" (weight: 0.007)\n",
      "- \"good\" (weight: 0.007)\n",
      "- \"time\" (weight: 0.007)\n",
      "- \"year\" (weight: 0.006)\n",
      "- \"could\" (weight: 0.006)\n",
      "\n",
      "Topic 1:\n",
      "- \"people\" (weight: 0.011)\n",
      "- \"would\" (weight: 0.009)\n",
      "- \"one\" (weight: 0.008)\n",
      "- \"government\" (weight: 0.006)\n",
      "- \"say\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"god\" (weight: 0.005)\n",
      "- \"u\" (weight: 0.005)\n",
      "- \"right\" (weight: 0.005)\n",
      "- \"law\" (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      "- \"key\" (weight: 0.015)\n",
      "- \"file\" (weight: 0.010)\n",
      "- \"use\" (weight: 0.010)\n",
      "- \"system\" (weight: 0.010)\n",
      "- \"chip\" (weight: 0.008)\n",
      "- \"encryption\" (weight: 0.007)\n",
      "- \"program\" (weight: 0.007)\n",
      "- \"window\" (weight: 0.006)\n",
      "- \"information\" (weight: 0.006)\n",
      "- \"data\" (weight: 0.005)\n",
      "\n",
      "Topic 3:\n",
      "- \"max\" (weight: 0.037)\n",
      "- \"db\" (weight: 0.018)\n",
      "- \"team\" (weight: 0.012)\n",
      "- \"university\" (weight: 0.007)\n",
      "- \"game\" (weight: 0.007)\n",
      "- \"year\" (weight: 0.006)\n",
      "- \"new\" (weight: 0.006)\n",
      "- \"space\" (weight: 0.005)\n",
      "- \"league\" (weight: 0.005)\n",
      "- \"season\" (weight: 0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10744d2f",
   "metadata": {},
   "source": [
    "## Calculate Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05266b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.5465\n"
     ]
    }
   ],
   "source": [
    "# import library for Coherence Score\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the coherence score for the LDA model\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "\n",
    "\n",
    "# Display the score\n",
    "\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0016bdd",
   "metadata": {},
   "source": [
    "This coherence score of 0.5465 for this topic model suggests that the topics are moderately coherent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
